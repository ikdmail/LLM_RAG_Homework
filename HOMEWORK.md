# LLM RAG 実装 宿題レポート

## 課題概要

本課題では、提供された架空の参照資料に基づき、RAG（検索拡張生成）システムを実装し、RAGなしのBase LLMと比較してその有効性を検証する。評価には、別のLLM（Gemini Flash）を用いた自動評価を行い、その結果を分析・考察する。

使用したLLM：Ollama (deepseek-r1:1.5b, llama3 など、実験で実際に使用したモデル名を記載)
使用したEmbeddingモデル：sentence-transformers (all-MiniLM-L6-v2)
評価者LLM：Google Gemini 1.5 Flash

## ○質問設計の観点と意図

本課題のために考案した10個の質問は、以下の観点と意図に基づいて設計しました。

1.  **参照資料のみに回答が存在すること:**
    * インターネット上の一般的な知識ではなく、提供された架空の「宇宙イノベーションズ株式会社」に関する`references/`ディレクトリ内のテキストファイルのみに回答が存在するような質問を作成しました。
    * これにより、RAGシステムが外部知識に依存せず、**検索によって取得した情報のみを活用できるか**、そしてBase LLMがその特定の情報を**知らないこと**を確認するのに適した評価設定となります。
    * 例: 特定のプロジェクト目標、特定の技術開発者、特定の社内規定など、資料固有の固有名詞や詳細情報に関する質問。

2.  **様々な回答形式を必要とすること:**
    * 単一の固有名詞や数値（例: 質問2, 3, 5）
    * 短いフレーズやキャッチコピー（例: 質問6）
    * 具体的な時間帯や手順（例: 質問7, 4）
    * 出来事の具体的な内容説明（例: 質問1, 8, 9, 10）
    * これらの異なる形式の回答を求めることで、RAGシステムが単にキーワードを抽出するだけでなく、コンテキストを理解し、適切な形式で回答を生成できるかを評価します。

3.  **RAGが有効に機能する可能性のある質問:**
    * 資料内に明確な記載がある質問（ほぼ全ての質問がこれに該当するよう意図）。
    * 特に、Base LLMが回答困難であろう、資料の特定の箇所にのみ書かれている質問。

これらの質問を通じて、RAGが参照資料という「外部知識源」を活用することで、Base LLM単体では回答できない質問に対してどれだけ正確・完全・適切に回答できるようになるか、というRAGの基本的な有効性を測定することを意図しました。

## ○RAGの実装方法と工夫点

RAGシステムは、主に以下のコンポーネントで実装しました。

1.  **参照資料の読み込み:**
    * `references/` ディレクトリ内の `.txt` および `.md` ファイルを読み込みます (`load_documents` 関数)。
    * 複数のファイルがある場合も対応できるようにしています。

2.  **ドキュメントの分割 (Chunking):**
    * 読み込んだドキュメントを、LLMのコンテキストウィンドウに収まるように小さな塊（チャンク）に分割します (`split_documents` 関数)。
    * シンプルな段落・改行ベースの分割に加え、チャンク間に一定の文字数（`CHUNK_OVERLAP`）の重複を持たせることで、チャンク境界での情報欠落を防ぐ工夫を行っています。
    * チャンクサイズ (`CHUNK_SIZE`) はLLMやEmbeddingモデルの特性に合わせて調整可能です。

3.  **ベクトルストアの構築:**
    * 分割した各チャンクを、`sentence-transformers` ライブラリを使用して数値ベクトル（埋め込み/Embedding）に変換します (`VectorStore` クラスの `add_chunks` メソッド)。
    * 使用したモデルは `all-MiniLM-L6-v2` です。これは比較的軽量で、Colab環境でも扱いやすいため選択しました。
    * これらの埋め込みと元のチャンクテキストをインメモリ（メモリ上）で保持するシンプルなベクトルストアを実装しました。

4.  **検索 (Retrieval):**
    * ユーザーからの質問テキストも同様にEmbeddingに変換します (`VectorStore` クラスの `retrieve` メソッド)。
    * 質問のEmbeddingと、全てのチャンクのEmbeddingとの間でコサイン類似度を計算し、類似度が高い上位 `NUM_RETRIEVED_CHUNKS` 個のチャンクを関連情報として取得します。

5.  **回答生成 (Generation):**
    * 取得した関連チャンクを結合し、元の質問とともにRAGプロンプトテンプレートにはめ込みます。
    * このプロンプトをOllamaで動作するLLMに渡して、最終的な回答を生成させます。プロンプトテンプレートは、LLMに「提供された参考情報に基づいて回答すること」を明確に指示しています。

**実装上の工夫点:**

* **モデル名の引数指定:** `base_llm_answer_generator.py` および `rag_answer_generator.py` を修正し、使用するOllamaモデル名をコマンドライン引数 `--model_name` で指定できるようにしました。これにより、スクリプトファイルを直接編集することなく、Colabノートブック上から異なるモデルに容易に切り替えて実験を実行できるようにしました。
* **ColabでのOllama利用:** Colab VM上でOllamaサーバーを起動し、モデルをプルして利用する手順を確立しました。これにより、ローカル環境の構築の手間なく、クラウド上で様々なOllamaモデルを試せるようにしました。
* **評価の自動化と複数プロンプト/リトライ:** `evaluate_answers.py` において、Gemini Flashを用いた自動評価を実装しました。評価者LLMへのプロンプトを複数用意し、それぞれの評価結果の平均を取ることで、LLM評価の不安定さを軽減する工夫をしました。また、APIエラー（レート制限や応答不良）やJSONパースエラーが発生した場合に自動的にリトライする機構を追加し、評価処理の完遂率を高めました。
* **評価結果の構造化:** 評価結果をCSVファイルとして出力する形式にし、後続の分析を容易にしました。

## ○結果の分析と考察

実験で得られた評価結果（CSVデータの分析より）は以下の通りです。

**平均評価スコアの比較 (0-5点)**

| 指標         | Base LLM (Ollama deepseek-r1:1.5b) 平均 | RAG 平均 | RAG vs Base (差) |
| :----------- | :-------------------------------------- | :------- | :--------------- |
| 正確性 (Accuracy) | 0.05                                    | 2.50     | +2.45            |
| 完全性 (Completeness) | 0.00                                    | 2.25     | +2.25            |
| 関連性 (Relevance) | 0.50                                    | 2.95     | +2.45            |
**(注: 上記スコアは提供されたCSVデータに基づく)**

分析から以下の点が明らかになりました。

1.  **Base LLM (deepseek-r1:1.5b) の限界:**
    * Base LLMは、多くの質問に対して「回答できません」という定型文を返すか、質問内容や参照資料と無関係な、しばしば不自然で多言語（中国語など）を含む回答を生成しました。
    * これは、この比較的小さなモデルが、今回の課題で意図した「参照資料固有の特定の知識」を学習しておらず、外部の情報源にアクセスする手段も持たないため、期待通りに機能しなかったことを明確に示しています。Base LLM単体では、この種の質問にはほとんど対応できません。

2.  **RAGによる性能向上:**
    * RAGを導入することで、全ての評価指標において平均スコアがBase LLMを大幅に上回りました。
    * 特に、参照資料内に明確な回答が短く記載されている質問（例: 質問1, 9, 10）では、RAGが高い正確性と完全性、関連性のスコアを獲得しており、RAGが参照資料から必要な情報を適切に検索・抽出・利用できていることが確認できました。
    * これにより、RAGがLLMに「知らない情報」を与えることで、回答能力を拡張できるという基本的な有効性が実証されました。

3.  **RAGの課題と考察:**
    * 一方で、RAGをもってしてもスコアが伸び悩む質問（例: 質問2, 3, 4, 5, 6, 8）も多く見られました。これらの質問に対するRAGの生成回答は、依然として不自然な表現、多言語の混入、`<think>` ブロックの表示、質問とは微妙にずれた内容などが含まれていました。
    * これは、主にRAGで使用している**基盤となるLLM（deepseek-r1:1.5b）の性能自体に限界がある**ためと考えられます。LLMが、検索によって与えられたコンテキスト情報（チャンク）をうまく理解し、それを統合して自然で正確な日本語の回答として再構築する能力が不足している可能性があります。特に、複雑な情報の組み合わせが必要な場合や、参照チャンクの質が高くない場合に、LLMの弱点が露呈しやすいと考えられます。
    * 検索（Retrieval）の精度も影響している可能性があります。質問によっては、Embeddingモデルが質問の意図を完全に捉えきれず、関連性の低いチャンクを抽出してしまうことで、LLMが誤った情報を基に回答を生成してしまうことが考えられます。
    * RAGプロンプトテンプレートの効果も、LLMの特性によって異なります。今回のプロンプトが `deepseek-r1:1.5b` に対して最適ではなかった可能性も否定できません。

4.  **評価プロセスの課題:**
    * Gemini Flashを用いた自動評価は、手動評価の手間を省く強力な手段ですが、レート制限や、評価者LLM自身の応答の不安定さ（JSONパースエラーや、評価基準の適用の一貫性のばらつき）といった課題も見られました。リトライ処理や待機時間の調整で完遂はできましたが、LLMによる評価結果の解釈には慎重さが必要であることが示唆されます。

**考察のまとめ:**

本実験を通じて、RAGはLLMが事前に学習していない特定のドキュメント知識を活用可能にし、回答能力を拡張する有効な手法であることを確認しました。しかし、その回答の最終的な質は、**検索精度の高さ**に加え、**基盤となるLLMが与えられたコンテキストをどれだけ高度に理解し、適切に自然言語で出力できるか**に強く依存します。今回使用した `deepseek-r1:1.5b` は、RAGのメリットを引き出すには十分な性能ではなかった可能性が高いです。

## ○発展的な改善案（任意）

本実験結果を踏まえ、RAGシステムの性能をさらに向上させるための発展的な改善案をいくつか提案します。

1.  **より高性能な基盤LLMの採用:**
    * 今回使用した `deepseek-r1:1.5b` よりも、パラメータ数が多く、日本語の生成能力や文脈理解能力が高いOllamaモデル（例: `llama3:8b`, `mistral:7b`, `gemma:7b`, `codeqwen:7b` など）を試すことが、回答品質向上に最も効果的であると考えられます。Colabの計算リソースの許す範囲で、より大きなモデルをテストします。

2.  **ドキュメント分割戦略の改善:**
    * シンプルな固定長チャンクだけでなく、文章構造や意味的なまとまりを考慮したより高度なチャンク分割手法（例: セマンティックチャンキング、再帰的なテキスト分割など）を導入することで、検索時の関連性を高められる可能性があります。

3.  **Embeddingモデルの検討:**
    * `all-MiniLM-L6-v2` は軽量ですが、より高度な意味理解能力を持つEmbeddingモデル（例: `multilingual-e5-large`, OpenAI Ada v2 など）を試すことで、検索精度が向上する可能性があります。ただし、計算リソースや推論速度とのトレードオフになります。

4.  **検索ロジックの強化:**
    * 質問とチャンクのEmbedding類似度だけでなく、キーワードマッチングなどを組み合わせたハイブリッド検索を導入する。
    * 取得した複数のチャンクを、別のLLMや専用のモデルで再ランキングし、最も関連性の高いチャンクを厳選するRe-ranking手法を取り入れる。

5.  **RAGプロンプトエンジニアリングの深化:**
    * 使用するLLMの特性に合わせて、RAGプロンプトテンプレートをより精緻に設計します。「忠実な回答」「簡潔さ」「特定の形式での出力」などをより効果的にLLMに伝えるための指示を検討します。
    * 例えば、few-shot学習として、質問・コンテキスト・理想的な回答のペアをいくつかプロンプトに含める方法などが考えられます。

6.  **評価手法の改善:**
    * LLM評価の不安定さやレート制限の問題に対して、評価者LLMの種類を検討したり、評価プロンプトをさらに洗練させたりします。
    * 可能であれば、少数の回答に対して手動評価を併用し、自動評価の妥当性を検証することも重要です。

7.  **LLMのファインチューニング:**
    * 最も高度なアプローチとして、特定のドキュメントセットやタスク（この課題のような社内規定に関するQAなど）に合わせて、基盤LLMをファインチューニングすることで、RAGの回答品質を大きく向上させられる可能性があります。これは大きな計算リソースとデータ準備が必要となります。

これらの改善案は、RAGシステムの各コンポーネント（検索、生成）および評価プロセスの質を高めることを目指すものです。特に、より高性能な基盤LLMへの変更は、RAGの有効性をより引き出す上で最も期待できる一歩と言えます。