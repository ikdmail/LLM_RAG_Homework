# evaluate_answers.py (Gemini Flash評価 & 平均スコア版 - 外部JSONから正解読み込み - リトライ対応)

import os
import re
import json
import pandas as pd
import time
import random # リトライ待機時間のばらつきのためにインポート

# --- Import necessary library for the Evaluator LLM ---
import google.generativeai as genai
from google.api_core import exceptions as google_exceptions # Google APIのエラーハンドリング用

# --- Configuration ---
REPO_ROOT = os.path.join("..")
QUESTIONS_FILE = os.path.join(REPO_ROOT, "questions", "questions.txt")
CORRECT_ANSWERS_FILE = os.path.join(REPO_ROOT, "questions", "correct_answers.json")
BASE_LLM_ANSWERS_FILE = os.path.join(REPO_ROOT, "results", "raw_answers_base_llm.md")
RAG_ANSWERS_FILE = os.path.join(REPO_ROOT, "results", "raw_answers_rag.md")
OUTPUT_DIR = os.path.join(REPO_ROOT, "results")
OUTPUT_EVALUATION_FILE = os.path.join(OUTPUT_DIR, "evaluation_results.csv")

# --- Automated Evaluation Settings ---
ENABLE_AUTOMATED_EVALUATION = True
EVALUATOR_API_KEY = os.getenv("EVALUATOR_GOOGLE_API_KEY")
EVALUATOR_LLM_MODEL = "gemini-1.5-flash-latest"

# 評価者LLMに与えるプロンプトテンプレート (2種類) - 内容は変更なし
EVALUATION_PROMPT_TEMPLATE_1 = """You are an AI assistant tasked with objectively evaluating answers generated by another AI model.
Evaluate the "Generated Answer" based on the "Question" and a provided "Correct Answer", using the specified "Evaluation Criteria".
Your primary goal is to objectively assess how well the Generated Answer factually aligns with the Correct Answer and directly addresses the Question. Do not use outside knowledge.

**Question:** {question}
**Correct Answer:** {correct_answer}
**Generated Answer:** {generated_answer}

**Evaluation Criteria (Score 0-5):**
- Accuracy (正確性): How factually correct is the Generated Answer based *specifically* on the Correct Answer? Does it hallucinate or contradict the Correct Answer? (0: Wholly incorrect or pure hallucination, 1: Mostly incorrect, 2: Partially correct but significant errors/omissions, 3: Mostly correct but minor errors or lack of detail, 4: Very accurate but might miss a subtle point, 5: Completely accurate and consistent with Correct Answer)
- Completeness (完全性): Does the Generated Answer include all *essential* pieces of information present in the Correct Answer that directly relate to the Question? (0: Includes almost none of the essential points, 1: Includes a tiny fraction, 2: Includes some but less than half, 3: Includes more than half, 4: Includes almost all essential points, 5: Includes all essential points)
- Relevance (関連性): Does the Generated Answer directly address the Question and avoid irrelevant information or going off-topic? (0: Wholly irrelevant or misunderstands the question, 1: Barely relevant, 2: Partially relevant but includes much irrelevant info, 3: Generally relevant but slightly off-focus or includes minor irrelevant info, 4: Highly relevant but might include a tiny bit of extra info, 5: Perfectly focused and relevant, no irrelevant information)

Provide your evaluation for the Generated Answer in the following JSON format. Ensure the JSON is valid and contains ONLY the JSON object.

```json
{{
  "Accuracy_Score": score,
  "Accuracy_Reason": "reason for accuracy score",
  "Completeness_Score": score,
  "Completeness_Reason": "reason for completeness score",
  "Relevance_Score": score,
  "Relevance_Reason": "reason for relevance score"
}}
```
"""

EVALUATION_PROMPT_TEMPLATE_2 = """Compare the "Generated Answer" to the "Correct Answer" based on the following criteria and score it from 0 to 5 for each. Be strict and objective based on the provided text.

Question: {question}
Correct Answer: {correct_answer}
Generated Answer: {generated_answer}

Score based on (0-5):
- Accuracy: How factually correct is it compared to the Correct Answer?
- Completeness: Does it cover all key points from the Correct Answer?
- Relevance: Does it directly answer the Question?

Provide your evaluation as a JSON object with scores (integers 0-5) and brief reasons.

```json
{{
  "Accuracy_Score": score,
  "Accuracy_Reason": "reason for accuracy",
  "Completeness_Score": score,
  "Completeness_Reason": "reason for completeness",
  "Relevance_Score": score,
  "Relevance_Reason": "reason for relevance"
}}
```
"""

EVALUATION_PROMPT_TEMPLATES = [
    EVALUATION_PROMPT_TEMPLATE_1,
    EVALUATION_PROMPT_TEMPLATE_2,
]

# API呼び出し間隔とリトライ設定
API_CALL_DELAY = 1.0 # 各API呼び出し間の基本的な待機時間 (秒)
DELAY_BETWEEN_QUESTIONS = 5.0 # 各質問の評価を終えてから次の質問に移るまでの待機時間 (秒)
MAX_RETRIES = 3 # API呼び出しまたはJSONパースが失敗した場合のリトライ回数
RETRY_DELAY_BASE = 5.0 # リトライ時の待機時間の基本値 (秒)


# --- Helper Functions ---

def load_correct_answers(filepath: str) -> dict[str, str]:
    """Loads correct answers from a JSON file."""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            correct_answers = json.load(f)
        print(f"Correct answers loaded successfully from {filepath}")
        if not all(isinstance(k, str) and isinstance(v, str) for k, v in correct_answers.items()):
             print("Warning: JSONファイルの内容が想定される形式（質問(string) -> 回答(string) の辞書）と異なる可能性があります。")
        return correct_answers
    except FileNotFoundError:
        print(f"エラー: Correct answers file not found at {filepath}")
        print("評価の正解を記述したJSONファイルが存在しません。パスを確認し、ファイルを作成し、正解を記述してください。")
        return {}
    except json.JSONDecodeError:
        print(f"エラー: Failed to parse JSON from correct answers file: {filepath}")
        print("JSONファイルの形式が正しくない可能性があります。JSON構文を確認してください。")
        return {}
    except Exception as e:
        print(f"Error loading correct answers file {filepath}: {e}")
        return {}


def parse_markdown_answers(filepath: str) -> dict[str, str]:
    """Parses answers from a Markdown file generated by the generator scripts."""
    answers = {}
    current_question = None
    current_answer_lines = []
    in_answer_block = False
    answer_markers = ["**ベースLLM回答:**", "**RAG回答:**"]

    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.rstrip()

                if re.match(r"^## 質問 \d+", line):
                    if current_question is not None and in_answer_block:
                        answers[current_question.strip()] = "\n".join(current_answer_lines).strip()
                    current_question = None
                    current_answer_lines = []
                    in_answer_block = False
                elif line.startswith("**質問:**"):
                    current_question = line[len("**質問:**"):].strip()
                elif any(line.startswith(marker) for marker in answer_markers):
                    if current_question is not None:
                         in_answer_block = True
                         current_answer_lines = []
                         remaining_line = line
                         for marker in answer_markers:
                             if remaining_line.startswith(marker):
                                 remaining_line = remaining_line[len(marker):].strip()
                                 break
                         current_answer_lines.append(remaining_line)
                elif in_answer_block:
                    if line.startswith("---"):
                        if current_question is not None:
                            answers[current_question.strip()] = "\n".join(current_answer_lines).strip()
                            current_question = None
                            current_answer_lines = []
                            in_answer_block = False
                    else:
                         current_answer_lines.append(line)

            if current_question is not None and in_answer_block:
                 answers[current_question.strip()] = "\n".join(current_answer_lines).strip()


    except FileNotFoundError:
        print(f"Error: Answer file not found at {filepath}")
        return {}
    except Exception as e:
        print(f"Error parsing file {filepath}: {e}")
        return {}

    if not answers:
        print(f"Warning: No answers parsed from {filepath}. Check file format.")

    return answers


def get_correct_answer(question: str, correct_answers_dict: dict) -> str:
    """Retrieves the correct answer for a given question from the loaded dictionary."""
    return correct_answers_dict.get(question, "【エラー】この質問の正解は定義されていません。")


# --- Automated Evaluation Function ---

def automated_evaluate(question: str, generated_answer: str, correct_answer: str, evaluator_llm_client, prompt_template: str, attempt: int) -> dict:
    """
    Uses an LLM to automatically evaluate a generated answer based on a specific prompt template.
    Includes retry logic.

    Args:
        question: The question string.
        generated_answer: The answer string to evaluate.
        correct_answer: The correct answer string for comparison.
        evaluator_llm_client: The initialized Google Generative AI client.
        prompt_template: The prompt template string to use.
        attempt: The current retry attempt number (0 for the first try).

    Returns:
        A dictionary of scores and reasons, or an error dict.
    """
    if evaluator_llm_client is None:
        return {"Error": "Evaluator LLM client not initialized."}

    prompt = prompt_template.format(
        question=question,
        correct_answer=correct_answer,
        generated_answer=generated_answer
    )

    try:
        # API呼び出し
        response = evaluator_llm_client.generate_content(prompt)

        # レスポンスが空でないか確認
        if not response or not response.text:
             print(f"Warning: Evaluator LLM returned empty response for question: {question[:50]}... (Attempt {attempt + 1})")
             return {"Error": "Empty response from evaluator LLM", "RawResponse": response.text if 'response' in locals() else 'N/A', "Prompt": prompt}


        # レスポンスからJSONブロックを抽出してパース
        json_match = re.search(r"```json(.*?)```", response.text, re.DOTALL)
        json_string = json_match.group(1).strip() if json_match else response.text.strip()

        evaluation_result = json.loads(json_string)

        # JSON構造とスコア範囲(0-5)の基本的な検証
        expected_keys = ["Accuracy_Score", "Accuracy_Reason", "Completeness_Score", "Completeness_Reason", "Relevance_Score", "Relevance_Reason"]
        if not all(key in evaluation_result for key in expected_keys):
             print(f"Warning: Evaluator LLM returned unexpected JSON keys for question: {question[:50]}... (Attempt {attempt + 1})")
             return {"Error": "Invalid JSON keys from evaluator LLM", "RawResponse": response.text, "ParsedJson": evaluation_result}

        # スコアを整数に変換し、範囲(0-5)を検証・クランプ
        for key in expected_keys:
             if key.endswith("_Score"):
                 try:
                     score = int(evaluation_result.get(key, 0))
                     if not (0 <= score <= 5):
                         print(f"Warning: Evaluator LLM returned score out of 0-5 range ({score}) for question: {question[:50]}... (Attempt {attempt + 1})")
                         score = max(0, min(5, score))
                     evaluation_result[key] = score
                 except (ValueError, TypeError):
                     print(f"Warning: Evaluator LLM returned non-integer score for question: {question[:50]}... (Attempt {attempt + 1})")
                     evaluation_result[key] = 0

        return evaluation_result

    except google_exceptions.QuotaExceeded as e:
         print(f"Quota Exceeded Error: {e} (Attempt {attempt + 1})")
         return {"Error": f"Quota Exceeded: {e}", "QuotaError": True} # レート制限エラーを示すフラグを追加
    except json.JSONDecodeError:
        print(f"Error: Failed to parse JSON from evaluator LLM response for question: {question[:50]}... (Attempt {attempt + 1})")
        # print(f"Raw response text (partial): {response.text[:500]}...")
        return {"Error": "JSON parsing failed", "RawResponse": response.text if 'response' in locals() else 'N/A', "Prompt": prompt}
    except Exception as e:
        print(f"Error during automated evaluation for question: {question[:50]}... (Attempt {attempt + 1})")
        print(f"Error details: {e}")
        return {"Error": f"Automated evaluation failed: {e}", "RawResponse": response.text if 'response' in locals() else 'N/A', "Prompt": prompt}


# --- Main Execution ---
if __name__ == "__main__":
    print(f"Colab環境で回答評価スクリプトを実行中 (Gemini Flash 評価者)...")
    print(f"カレントディレクトリ: {os.getcwd()}")

    # --- 0. 正解データの読み込み ---
    print(f"'{CORRECT_ANSWERS_FILE}' から正解データを読み込んでいます...")
    correct_answers_dict = load_correct_answers(CORRECT_ANSWERS_FILE)
    if not correct_answers_dict:
        print("エラー: 正解データが読み込めませんでした。処理を中断します。")
        exit()
    print(f"{len(correct_answers_dict)} 件の正解データを読み込みました。")


    # --- 1. 評価者LLMの初期化 (自動評価を有効な場合) ---
    evaluator_llm_client = None
    if ENABLE_AUTOMATED_EVALUATION:
        print(f"\n評価者LLM ({EVALUATOR_LLM_MODEL}) を初期化します...")
        if not EVALUATOR_API_KEY:
             print("エラー: 評価者LLM用のAPIキーが設定されていません。環境変数 'EVALUATOR_GOOGLE_API_KEY' を設定してください。")
             print("自動評価は実行されません。")
             ENABLE_AUTOMATED_EVALUATION = False
        else:
            try:
                genai.configure(api_key=EVALUATOR_API_KEY)
                safety_settings = [
                    {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
                    {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
                    {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
                    {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
                ]
                evaluator_llm_client = genai.GenerativeModel(EVALUATOR_LLM_MODEL, safety_settings=safety_settings)
                evaluator_llm_client.generate_content("Test", stream=False) # 簡単な確認
                print(f"評価者LLMクライアント ({EVALUATOR_LLM_MODEL}) の初期化に成功しました。")
            except Exception as e:
                print(f"エラー: 評価者LLMクライアント ({EVALUATOR_LLM_MODEL}) の初期化に失敗しました。")
                print("APIキーが正しいか、または指定したモデルが利用可能か確認してください。")
                print(f"詳細: {e}")
                print("自動評価は実行されません。")
                ENABLE_AUTOMATED_EVALUATION = False


    # --- 2. 生成された回答の読み込み ---
    print(f"\n生成された回答ファイルを読み込みます...")
    base_llm_answers = parse_markdown_answers(BASE_LLM_ANSWERS_FILE)
    rag_answers = parse_markdown_answers(RAG_ANSWERS_FILE)

    if not base_llm_answers and not rag_answers:
        print("エラー: 読み込める回答ファイルが見つかりませんでした。生成スクリプトが正常に完了したか確認してください。")
        exit()

    questions_to_evaluate = list(correct_answers_dict.keys())

    if not questions_to_evaluate:
         print("エラー: 評価対象となる質問リストが取得できませんでした。正解ファイルに質問と正解が定義されているか確認してください。")
         exit()

    print(f"評価対象となる {len(questions_to_evaluate)} 件の質問を評価します。")
    for q in questions_to_evaluate:
        if q not in base_llm_answers:
            print(f"警告: Base LLM 回答ファイルに質問 '{q}' が見つかりません。")
        if q not in rag_answers:
            print(f"警告: RAG 回答ファイルに質問 '{q}' が見つかりません。")


    # --- 3. 評価の実行 ---
    print(f"\n回答の評価を実行します (自動評価: {ENABLE_AUTOMATED_EVALUATION})...")
    evaluation_results_list = []

    for i, question in enumerate(questions_to_evaluate):
        print(f"\n--- 質問 {i+1}/{len(questions_to_evaluate)} の評価 ---")
        print(f"質問: {question[:80]}...") # 質問が長い場合を考慮して先頭だけ表示

        correct_ans = get_correct_answer(question, correct_answers_dict)
        base_ans = base_llm_answers.get(question, "【回答なし】")
        rag_ans = rag_answers.get(question, "【回答なし】")


        result_entry = {
            "Question": question,
            "Correct_Answer": correct_ans,
            "BaseLLM_Answer": base_ans,
            "RAG_Answer": rag_ans,
            "BaseLLM_Accuracy_Score_Avg": "N/A", "BaseLLM_Completeness_Score_Avg": "N/A", "BaseLLM_Relevance_Score_Avg": "N/A",
            "RAG_Accuracy_Score_Avg": "N/A", "RAG_Completeness_Score_Avg": "N/A", "RAG_Relevance_Score_Avg": "N/A",
            "BaseLLM_Evaluation_Notes": "",
            "RAG_Evaluation_Notes": "",
        }

        if ENABLE_AUTOMATED_EVALUATION and evaluator_llm_client:
            prompt_templates = EVALUATION_PROMPT_TEMPLATES
            base_scores_list = []
            rag_scores_list = []
            base_reasons = []
            rag_reasons = []

            print(f"  -> Running automated evaluation using {len(prompt_templates)} prompts...")

            for prompt_idx, prompt_template in enumerate(prompt_templates):
                print(f"    -> Evaluating with Prompt Template {prompt_idx + 1}...")

                # Base LLM回答の評価 (リトライ付き)
                print("      -> BaseLLM...")
                eval_success = False
                for attempt in range(MAX_RETRIES):
                    eval_result = automated_evaluate(question, base_ans, correct_ans, evaluator_llm_client, prompt_template, attempt)
                    if "Error" not in eval_result:
                        base_scores_list.append({
                            "Accuracy_Score": eval_result.get("Accuracy_Score", 0),
                            "Completeness_Score": eval_result.get("Completeness_Score", 0),
                            "Relevance_Score": eval_result.get("Relevance_Score", 0),
                        })
                        base_reasons.append(f"P{prompt_idx+1} Attempt{attempt+1}: Acc({str(eval_result.get('Accuracy_Reason', 'N/A'))[:50]}), Comp({str(eval_result.get('Completeness_Reason', 'N/A'))[:50]}), Rel({str(eval_result.get('Relevance_Reason', 'N/A'))[:50]})")
                        print(f"        -> Success on attempt {attempt + 1}.")
                        eval_success = True
                        break # 成功したらリトライループを抜ける
                    else:
                        # 失敗した場合
                        reason_note = f"P{prompt_idx+1} Attempt{attempt+1}: Error - {eval_result.get('Error', 'Unknown')[:100]}"
                        print(f"        -> Failed on attempt {attempt + 1} (Error: {eval_result.get('Error', 'Unknown')}).")
                        if attempt < MAX_RETRIES - 1:
                            # レート制限エラーの場合、少し長めに待つ
                            wait_time = RETRY_DELAY_BASE + random.uniform(0, 2) # 少しランダム性を加える
                            if eval_result.get("QuotaError"):
                                print(f"        -> Quota error, waiting {wait_time:.2f} seconds before retry...")
                                time.sleep(wait_time)
                            else:
                                # その他のエラーの場合は基本的な待機
                                print(f"        -> Waiting {API_CALL_DELAY:.2f} seconds before retry...")
                                time.sleep(API_CALL_DELAY)
                        else:
                             # 最終リトライも失敗
                             base_reasons.append(reason_note) # 最後の失敗理由を記録


                # RAG回答の評価 (リトライ付き)
                print("      -> RAG...")
                eval_success = False
                for attempt in range(MAX_RETRIES):
                    eval_result = automated_evaluate(question, rag_ans, correct_ans, evaluator_llm_client, prompt_template, attempt)
                    if "Error" not in eval_result:
                         rag_scores_list.append({
                            "Accuracy_Score": eval_result.get("Accuracy_Score", 0),
                            "Completeness_Score": eval_result.get("Completeness_Score", 0),
                            "Relevance_Score": eval_result.get("Relevance_Score", 0),
                        })
                         rag_reasons.append(f"P{prompt_idx+1} Attempt{attempt+1}: Acc({str(eval_result.get('Accuracy_Reason', 'N/A'))[:50]}), Comp({str(eval_result.get('Completeness_Reason', 'N/A'))[:50]}), Rel({str(eval_result.get('Relevance_Reason', 'N/A'))[:50]})")
                         print(f"        -> Success on attempt {attempt + 1}.")
                         eval_success = True
                         break # 成功したらリトライループを抜ける
                    else:
                        # 失敗した場合
                        reason_note = f"P{prompt_idx+1} Attempt{attempt+1}: Error - {eval_result.get('Error', 'Unknown')[:100]}"
                        print(f"        -> Failed on attempt {attempt + 1} (Error: {eval_result.get('Error', 'Unknown')}).")
                        if attempt < MAX_RETRIES - 1:
                            wait_time = RETRY_DELAY_BASE + random.uniform(0, 2) # 少しランダム性を加える
                            if eval_result.get("QuotaError"):
                                print(f"        -> Quota error, waiting {wait_time:.2f} seconds before retry...")
                                time.sleep(wait_time)
                            else:
                                print(f"        -> Waiting {API_CALL_DELAY:.2f} seconds before retry...")
                                time.sleep(API_CALL_DELAY)
                        else:
                            rag_reasons.append(reason_note)

                # --- プロンプト間のAPI呼び出し間隔 ---
                # 同じ質問・同じ回答者に対して、プロンプトを変えて呼び出す間の待機
                if prompt_idx < len(prompt_templates) - 1:
                     print(f"    -> Waiting {API_CALL_DELAY:.2f} seconds before next prompt...")
                     time.sleep(API_CALL_DELAY)


            # --- 平均スコアの計算と結果への格納 ---
            # base_scores_list には成功した評価のスコアのみが入っている
            if base_scores_list:
                avg_base_scores = pd.DataFrame(base_scores_list).mean().to_dict()
                result_entry["BaseLLM_Accuracy_Score_Avg"] = round(avg_base_scores.get("Accuracy_Score", 0), 2)
                result_entry["BaseLLM_Completeness_Score_Avg"] = round(avg_base_scores.get("Completeness_Score", 0), 2)
                result_entry["BaseLLM_Relevance_Score_Avg"] = round(avg_base_scores.get("Relevance_Score", 0), 2)
                result_entry["BaseLLM_Evaluation_Notes"] = f"Avg over {len(base_scores_list)}/{len(prompt_templates)*MAX_RETRIES} total attempts. Reasons: {' | '.join(base_reasons)}"
            else:
                 result_entry["BaseLLM_Evaluation_Notes"] = f"Eval failed for all {len(prompt_templates)*MAX_RETRIES} attempts. Reasons: {' | '.join(base_reasons)}"

            if rag_scores_list:
                avg_rag_scores = pd.DataFrame(rag_scores_list).mean().to_dict()
                result_entry["RAG_Accuracy_Score_Avg"] = round(avg_rag_scores.get("Accuracy_Score", 0), 2)
                result_entry["RAG_Completeness_Score_Avg"] = round(avg_rag_scores.get("Completeness_Score", 0), 2)
                result_entry["RAG_Relevance_Score_Avg"] = round(avg_rag_scores.get("Relevance_Score", 0), 2)
                result_entry["RAG_Evaluation_Notes"] = f"Avg over {len(rag_scores_list)}/{len(prompt_templates)*MAX_RETRIES} total attempts. Reasons: {' | '.join(rag_reasons)}"
            else:
                result_entry["RAG_Evaluation_Notes"] = f"Eval failed for all {len(prompt_templates)*MAX_RETRIES} attempts. Reasons: {' | '.join(rag_reasons)}"


        # TODO: 必要なら手動評価をここに追加

        evaluation_results_list.append(result_entry)

        # --- 質問間のAPI呼び出し間隔 ---
        # 1つの質問に対する評価を終えてから次の質問に移るまでの待機
        if i < len(questions_to_evaluate) - 1:
            print(f"\nWaiting {DELAY_BETWEEN_QUESTIONS:.2f} seconds before next question...")
            time.sleep(DELAY_BETWEEN_QUESTIONS)


    print("\n全ての質問の評価が完了しました。")

    # --- 4. 結果の保存 ---
    print(f"\n評価結果を '{OUTPUT_EVALUATION_FILE}' に保存します...")
    try:
        os.makedirs(OUTPUT_DIR, exist_ok=True)

        df_results = pd.DataFrame(evaluation_results_list)
        column_order = [
            "Question", "Correct_Answer", "BaseLLM_Answer", "RAG_Answer",
            "BaseLLM_Accuracy_Score_Avg", "BaseLLM_Completeness_Score_Avg", "BaseLLM_Relevance_Score_Avg",
            "RAG_Accuracy_Score_Avg", "RAG_Completeness_Score_Avg", "RAG_Relevance_Score_Avg",
            "BaseLLM_Evaluation_Notes", "RAG_Evaluation_Notes",
        ]
        df_results = df_results[ [col for col in column_order if col in df_results.columns] ]

        df_results.to_csv(OUTPUT_EVALUATION_FILE, index=False, encoding='utf-8')

        print(f"評価結果が正常に保存されました: {OUTPUT_EVALUATION_FILE}")

    except Exception as e:
        print(f"エラー: 評価結果の保存中にエラーが発生しました: {e}")