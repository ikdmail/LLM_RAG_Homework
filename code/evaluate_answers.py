# evaluate_answers.py (Gemini Flash評価 & 平均スコア版 - 外部JSONから正解読み込み - リトライ対応)

import os
import re
import json # JSONファイルを扱うためにインポート
import pandas as pd # 結果整理とCSV出力のためにインポート
import time # 評価間隔のためのインポート
import random # リトライ待機時間のばらつきのためにインポート

# --- Import necessary library for the Evaluator LLM ---
# Google Generative AI (Gemini) を使用する場合
# インストールが必要です: pip install google-generativeai
import google.generativeai as genai
# Google APIのエラーハンドリング用。QuotaExceededはResourceExhaustedの子クラス。
from google.api_core import exceptions as google_exceptions

# --- Configuration ---
# Paths (assuming script is run from the 'code' directory)
# スクリプトが code/ ディレクトリにあることを前提とした相対パスです。
REPO_ROOT = os.path.join("..") # code/ の一つ上の階層 (リポジトリのルート)
QUESTIONS_FILE = os.path.join(REPO_ROOT, "questions", "questions.txt")
# 評価の正解を記述したJSONファイルのパス
# questions/ ディレクトリに correct_answers.json を配置してください。
CORRECT_ANSWERS_FILE = os.path.join(REPO_ROOT, "questions", "correct_answers.json") # <-- 正解ファイルパス
BASE_LLM_ANSWERS_FILE = os.path.join(REPO_ROOT, "results", "raw_answers_base_llm.md")
RAG_ANSWERS_FILE = os.path.join(REPO_ROOT, "results", "raw_answers_rag.md")
OUTPUT_DIR = os.path.join(REPO_ROOT, "results") # 評価結果を保存するディレクトリ
OUTPUT_EVALUATION_FILE = os.path.join(OUTPUT_DIR, "evaluation_results.csv") # 評価結果CSVファイル名

# --- Automated Evaluation Settings ---
# TODO: 自動評価を行う場合、以下の設定を調整してください。
ENABLE_AUTOMATED_EVALUATION = True # TrueにするとLLMによる自動評価を実行 (Falseにすると評価は行われません)
# 評価者として使うLLMの設定 (テスト対象のLLMとは別のキー/モデル推奨)
# Colabの環境変数またはシークレットで設定してください。
EVALUATOR_API_KEY = os.getenv("EVALUATOR_GOOGLE_API_KEY") # 例: 環境変数名 EVALUATOR_GOOGLE_API_KEY から読み込む
EVALUATOR_LLM_MODEL = "gemini-1.5-flash-latest" # 評価者として使うGemini Flashモデル名

# 評価者LLMに与えるプロンプトテンプレート (2種類)
# これらはLLMが評価の基準と形式を理解するために使用されます。
# プロンプト1: 詳細な構造指定
EVALUATION_PROMPT_TEMPLATE_1 = """You are an AI assistant tasked with objectively evaluating answers generated by another AI model.
Evaluate the "Generated Answer" based on the "Question" and a provided "Correct Answer", using the specified "Evaluation Criteria".
Your primary goal is to objectively assess how well the Generated Answer factually aligns with the Correct Answer and directly addresses the Question. Do not use outside knowledge.

**Question:** {question}
**Correct Answer:** {correct_answer}
**Generated Answer:** {generated_answer}

**Evaluation Criteria (Score 0-5):**
- Accuracy (正確性): How factually correct is the Generated Answer based *specifically* on the Correct Answer? Does it hallucinate or contradict the Correct Answer? (0: Wholly incorrect or pure hallucination, 1: Mostly incorrect, 2: Partially correct but significant errors/omissions, 3: Mostly correct but minor errors or lack of detail, 4: Very accurate but might miss a subtle point, 5: Completely accurate and consistent with Correct Answer)
- Completeness (完全性): Does the Generated Answer include all *essential* pieces of information present in the Correct Answer that directly relate to the Question? (0: Includes almost none of the essential points, 1: Includes a tiny fraction, 2: Includes some but less than half, 3: Includes more than half, 4: Includes almost all essential points, 5: Includes all essential points)
- Relevance (関連性): Does the Generated Answer directly address the Question and avoid irrelevant information or going off-topic? (0: Wholly irrelevant or misunderstands the question, 1: Barely relevant, 2: Partially relevant but includes much irrelevant info, 3: Generally relevant but slightly off-focus or includes minor irrelevant info, 4: Highly relevant but might include a tiny bit of extra info, 5: Perfectly focused and relevant, no irrelevant information)

Provide your evaluation for the Generated Answer in the following JSON format. Ensure the JSON is valid and contains ONLY the JSON object.

```json
{{
  "Accuracy_Score": score,
  "Accuracy_Reason": "reason for accuracy score",
  "Completeness_Score": score,
  "Completeness_Reason": "reason for completeness score",
  "Relevance_Score": score,
  "Relevance_Reason": "reason for relevance score"
}}
```
"""

# プロンプト2: シンプルな比較依頼
EVALUATION_PROMPT_TEMPLATE_2 = """Compare the "Generated Answer" to the "Correct Answer" based on the following criteria and score it from 0 to 5 for each. Be strict and objective based on the provided text.

Question: {question}
Correct Answer: {correct_answer}
Generated Answer: {generated_answer}

Score based on (0-5):
- Accuracy: How factually correct is it compared to the Correct Answer?
- Completeness: Does it cover all key points from the Correct Answer?
- Relevance: Does it directly answer the Question?

Provide your evaluation as a JSON object with scores (integers 0-5) and brief reasons.

```json
{{
  "Accuracy_Score": score,
  "Accuracy_Reason": "reason for accuracy",
  "Completeness_Score": score,
  "Completeness_Reason": "reason for completeness",
  "Relevance_Score": score,
  "Relevance_Reason": "reason for relevance"
}}
```
"""

# 使用する評価プロンプトテンプレートのリスト
# ここに定義した全てのプロンプトで評価を実行し、平均スコアを計算します。
EVALUATION_PROMPT_TEMPLATES = [
    EVALUATION_PROMPT_TEMPLATE_1,
    EVALUATION_PROMPT_TEMPLATE_2,
    # 必要であれば3つ目のプロンプトなどを追加することも可能です。
]

# API呼び出し間隔とリトライ設定
API_CALL_DELAY = 1.0 # 各API呼び出し間の基本的な待機時間 (秒)
DELAY_BETWEEN_QUESTIONS = 7.0 # 各質問の評価を終えてから次の質問に移るまでの待機時間 (秒) - レート制限回避のため調整
MAX_RETRIES = 3 # API呼び出しまたはJSONパースが失敗した場合のリトライ回数
RETRY_DELAY_BASE = 5.0 # リトライ時の待機時間の基本値 (秒)


# --- Helper Functions ---

def load_correct_answers(filepath: str) -> dict[str, str]:
    """Loads correct answers from a JSON file."""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            correct_answers = json.load(f)
        print(f"Correct answers loaded successfully from {filepath}")
        if not all(isinstance(k, str) and isinstance(v, str) for k, v in correct_answers.items()):
             print("Warning: JSONファイルの内容が想定される形式（質問(string) -> 回答(string) の辞書）と異なる可能性があります。")
             # 警告は出すが処理は続行
        return correct_answers
    except FileNotFoundError:
        print(f"エラー: Correct answers file not found at {filepath}")
        print("評価の正解を記述したJSONファイルが存在しません。パスを確認し、ファイルを作成し、正解を記述してください。")
        return {}
    except json.JSONDecodeError:
        print(f"エラー: Failed to parse JSON from correct answers file: {filepath}")
        print("JSONファイルの形式が正しくない可能性があります。JSON構文を確認してください。")
        return {}
    except Exception as e:
        print(f"Error loading correct answers file {filepath}: {e}")
        return {}


def parse_markdown_answers(filepath: str) -> dict[str, str]:
    """Parses answers from a Markdown file generated by the generator scripts."""
    answers = {}
    current_question = None
    current_answer_lines = []
    in_answer_block = False
    # 回答ブロックの開始マーカーリスト。スクリプト出力に合わせて調整してください。
    answer_markers = ["**ベースLLM回答:**", "**RAG回答:**"]

    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.rstrip() # 行末の改行コードは削除

                # ## 質問 N という行で新しい質問ブロックの開始を検出
                if re.match(r"^## 質問 \d+", line):
                    # 前の質問と回答があれば保存
                    if current_question is not None and in_answer_block:
                        answers[current_question.strip()] = "\n".join(current_answer_lines).strip()
                    current_question = None # 質問情報をリセット
                    current_answer_lines = []
                    in_answer_block = False
                # **質問:** という行で質問テキストを抽出
                elif line.startswith("**質問:**"):
                    # Markdownの太字マーカーを含めて除去
                    current_question = line[len("**質問:**"):].strip()
                # 回答開始マーカーのいずれかで回答ブロックの開始を検出
                elif any(line.startswith(marker) for marker in answer_markers):
                    if current_question is not None: # 質問が抽出済みの場合のみ回答として扱う
                         in_answer_block = True
                         current_answer_lines = [] # 回答行リストをクリア
                         # マーカー自体を回答の最初の行から除去
                         remaining_line = line
                         for marker in answer_markers:
                             if remaining_line.startswith(marker):
                                 remaining_line = remaining_line[len(marker):].strip()
                                 break
                         current_answer_lines.append(remaining_line) # マーカーを除去した行を追加
                # 回答ブロック内の行を収集（--- または 次の ##質問 まで）
                elif in_answer_block:
                    # セパレーター(---)で回答ブロックの終了を検出
                    if line.startswith("---"):
                        if current_question is not None:
                            answers[current_question.strip()] = "\n".join(current_answer_lines).strip()
                            current_question = None # 質問情報をリセット
                            current_answer_lines = []
                            in_answer_block = False
                    else:
                         current_answer_lines.append(line)

            # ファイルの最後に---がない場合や、最後の質問の回答を保存
            if current_question is not None and in_answer_block:
                 answers[current_question.strip()] = "\n".join(current_answer_lines).strip()


    except FileNotFoundError:
        print(f"Error: Answer file not found at {filepath}")
        return {}
    except Exception as e:
        print(f"Error parsing file {filepath}: {e}")
        return {}

    # 基本的な検証
    if not answers:
        print(f"Warning: No answers parsed from {filepath}. Check file format or content.")

    return answers


def get_correct_answer(question: str, correct_answers_dict: dict) -> str:
    """Retrieves the correct answer for a given question from the loaded dictionary."""
    # 正解辞書から該当する質問の正解を取得
    # 質問テキストがキーとして完全に一致する必要があります
    return correct_answers_dict.get(question, "【エラー】この質問の正解は定義されていません。")


# --- Automated Evaluation Function ---

def automated_evaluate(question: str, generated_answer: str, correct_answer: str, evaluator_llm_client, prompt_template: str, attempt: int) -> dict:
    """
    Uses an LLM to automatically evaluate a generated answer based on a specific prompt template.
    Includes retry logic.

    Args:
        question: The question string.
        generated_answer: The answer string to evaluate.
        correct_answer: The correct answer string for comparison.
        evaluator_llm_client: The initialized Google Generative AI client.
        prompt_template: The prompt template string to use.
        attempt: The current retry attempt number (0 for the first try).

    Returns:
        A dictionary of scores and reasons, or an error dict.
    """
    if evaluator_llm_client is None:
        return {"Error": "Evaluator LLM client not initialized."}

    # プロンプトをテンプレートと情報でフォーマット
    prompt = prompt_template.format(
        question=question,
        correct_answer=correct_answer,
        generated_answer=generated_answer
    )

    try:
        # API呼び出し
        response = evaluator_llm_client.generate_content(prompt)

        # レスポンスが空でないか確認
        if not response or not response.text:
             print(f"Warning: Evaluator LLM returned empty response for question: {question[:50]}... (Attempt {attempt + 1})")
             return {"Error": "Empty response from evaluator LLM", "RawResponse": response.text if 'response' in locals() else 'N/A', "Prompt": prompt}


        # レスポンスからJSONブロックを抽出してパース
        # 最初の ```json ``` ブロックを正規表現で検索
        json_match = re.search(r"```json(.*?)```", response.text, re.DOTALL)
        json_string = json_match.group(1).strip() if json_match else response.text.strip() # 見つからなければ全文を試す

        # JSONをパース
        evaluation_result = json.loads(json_string)

        # JSON構造とスコア範囲(0-5)の基本的な検証
        expected_keys = ["Accuracy_Score", "Accuracy_Reason", "Completeness_Score", "Completeness_Reason", "Relevance_Score", "Relevance_Reason"]
        # 必須キーが全て存在するかチェック
        if not all(key in evaluation_result for key in expected_keys):
             print(f"Warning: Evaluator LLM returned unexpected JSON keys for question: {question[:50]}... (Attempt {attempt + 1})")
             # 期待するキーがない場合はエラーとして扱い、評価失敗とする
             return {"Error": "Invalid JSON keys from evaluator LLM", "RawResponse": response.text, "ParsedJson": evaluation_result}

        # スコアを整数に変換し、範囲(0-5)を検証・クランプ
        for key in expected_keys:
             if key.endswith("_Score"):
                 try:
                     score = int(evaluation_result.get(key, 0))
                     if not (0 <= score <= 5):
                         # 範囲外の場合は警告を出し、0-5の間に収める
                         print(f"Warning: Evaluator LLM returned score out of 0-5 range ({score}) for question: {question[:50]}... (Attempt {attempt + 1})")
                         score = max(0, min(5, score)) # スコアを0-5にクランプ
                     evaluation_result[key] = score
                 except (ValueError, TypeError):
                     # 数値に変換できない場合は警告を出し、スコアを0とする
                     print(f"Warning: Evaluator LLM returned non-integer score for question: {question[:50]}... (Attempt {attempt + 1})")
                     evaluation_result[key] = 0 # 無効なスコアは0とする

        return evaluation_result

    # レート制限など、リトライ可能なGoogle API関連のエラーを捕獲
    # ResourceExhaustedは429エラーを示し、QuotaExceededもこれに含まれる
    except google_exceptions.ResourceExhausted as e: # <-- ここを修正
         print(f"Resource Exhausted Error (likely Quota Exceeded): {e} (Attempt {attempt + 1})")
         # これはレート制限エラーなので、QuotaErrorフラグを立てて、リトライが必要であることを示す
         return {"Error": f"Resource Exhausted: {e}", "QuotaError": True, "RawResponse": response.text if 'response' in locals() else 'N/A'}
    except json.JSONDecodeError:
        # JSONパースエラーが発生した場合
        print(f"Error: Failed to parse JSON from evaluator LLM response for question: {question[:50]}... (Attempt {attempt + 1})")
        # print(f"Raw response text (partial): {response.text[:500]}...") # 生のレスポンスの一部を表示（デバッグ用）
        return {"Error": "JSON parsing failed", "RawResponse": response.text if 'response' in locals() else 'N/A', "Prompt": prompt}
    except Exception as e:
        # その他の予期しないエラーが発生した場合
        print(f"Error during automated evaluation for question: {question[:50]}... (Attempt {attempt + 1})")
        print(f"Error details: {e}")
        return {"Error": f"Automated evaluation failed: {e}", "RawResponse": response.text if 'response' in locals() else 'N/A', "Prompt": prompt}


# TODO: 必要であれば手動評価関数を実装します。
# Colabのコードセルでインタラクティブな入力を扱うのは少し工夫が必要です。
# ローカルPCでスクリプトを実行する場合に向いています。
# def manual_evaluate(question: str, generated_answer: str, correct_answer: str):
#      print("\n--- 手動評価 ---")
#      print(f"質問: {question}")
#      print(f"正解: {correct_answer}")
#      print(f"生成回答: {generated_answer}")
#      print("---")
#      print("以下の基準で0-5点のスコアを入力してください。")
#
#      scores = {}
#      criteria = ["正確性", "完全性", "関連性"] # 手動で評価する基準
#
#      for criterion in criteria:
#          while True:
#              try:
#                  score = int(input(f"{criterion} スコア (0-5): "))
#                  if 0 <= score <= 5:
#                      scores[criterion] = score
#                      break
#                  else:
#                      print("0から5の間の数値を入力してください。")
#              except ValueError:
#                  print("無効な入力です。数値を入力してください。")
#
#      return scores


# --- Main Execution ---
if __name__ == "__main__":
    print(f"Colab環境で回答評価スクリプトを実行中 (Gemini Flash 評価者)...")
    print(f"カレントディレクトリ: {os.getcwd()}") # Colabでのカレントディレクトリ確認用

    # --- 0. 正解データの読み込み ---
    print(f"'{CORRECT_ANSWERS_FILE}' から正解データを読み込んでいます...")
    correct_answers_dict = load_correct_answers(CORRECT_ANSWERS_FILE)
    if not correct_answers_dict:
        print("エラー: 正解データが読み込めませんでした。処理を中断します。")
        exit()
    print(f"{len(correct_answers_dict)} 件の正解データを読み込みました。")


    # --- 1. 評価者LLMの初期化 (自動評価を有効な場合) ---
    evaluator_llm_client = None
    if ENABLE_AUTOMATED_EVALUATION:
        print(f"\n評価者LLM ({EVALUATOR_LLM_MODEL}) を初期化します...")
        if not EVALUATOR_API_KEY:
             print("エラー: 評価者LLM用のAPIキーが設定されていません。環境変数 'EVALUATOR_GOOGLE_API_KEY' を設定してください。")
             print("自動評価は実行されません。")
             ENABLE_AUTOMATED_EVALUATION = False # APIキーがない場合は自動評価を無効化
        else:
            try:
                genai.configure(api_key=EVALUATOR_API_KEY)
                # Safety settings are important for production, but often less strict for evaluation tasks
                # You might want to adjust these based on the content being evaluated
                safety_settings = [
                    {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
                    {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
                    {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
                    {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
                ]
                evaluator_llm_client = genai.GenerativeModel(EVALUATOR_LLM_MODEL, safety_settings=safety_settings)
                # 簡単な確認 - generate_content の失敗をキャッチできるように修正
                try:
                    evaluator_llm_client.generate_content("Test", stream=False)
                    print(f"評価者LLMクライアント ({EVALUATOR_LLM_MODEL}) の初期化に成功しました。")
                except Exception as e:
                     print(f"警告: 評価者LLMクライアントの確認呼び出し中にエラー: {e}")
                     print("APIキーは設定されているようですが、モデル呼び出しに問題がある可能性があります。続行します。")


            except Exception as e:
                print(f"エラー: 評価者LLMクライアント ({EVALUATOR_LLM_MODEL}) の初期化に失敗しました。")
                print("APIキーが正しいか、または指定したモデルが利用可能か確認してください。")
                print(f"詳細: {e}")
                print("自動評価は実行されません。")
                ENABLE_AUTOMATED_EVALUATION = False # 初期化失敗時は自動評価を無効化


    # --- 2. 生成された回答の読み込み ---
    print(f"\n生成された回答ファイルを読み込みます...")
    base_llm_answers = parse_markdown_answers(BASE_LLM_ANSWERS_FILE)
    rag_answers = parse_markdown_answers(RAG_ANSWERS_FILE)

    if not base_llm_answers and not rag_answers:
        print("エラー: 読み込める回答ファイルが見つかりませんでした。生成スクリプトが正常に完了したか確認してください。")
        # 回答ファイルがない場合は評価できないので終了
        exit()

    # 評価対象となる質問リストは正解データから取得 (正解が定義されている質問のみを評価)
    questions_to_evaluate = list(correct_answers_dict.keys())

    if not questions_to_evaluate:
         print("エラー: 評価対象となる質問リストが取得できませんでした。正解ファイルに質問と正解が定義されているか確認してください。")
         exit()

    print(f"評価対象となる {len(questions_to_evaluate)} 件の質問を評価します。")
    # 念のため、生成された回答ファイルに評価対象の質問が含まれているか確認
    for q in questions_to_evaluate:
        if q not in base_llm_answers:
            print(f"警告: Base LLM 回答ファイルに質問 '{q}' が見つかりません。")
        if q not in rag_answers:
            print(f"警告: RAG 回答ファイルに質問 '{q}' が見つかりません。")


    # --- 3. 評価の実行 ---
    print(f"\n回答の評価を実行します (自動評価: {ENABLE_AUTOMATED_EVALUATION})...")
    evaluation_results_list = []

    # 評価対象の質問リストをループ
    for i, question in enumerate(questions_to_evaluate):
        print(f"\n--- 質問 {i+1}/{len(questions_to_evaluate)} の評価 ---")
        print(f"質問: {question[:80]}...") # 質問が長い場合を考慮して先頭だけ表示

        correct_ans = get_correct_answer(question, correct_answers_dict) # 辞書を渡す
        # 生成された回答ファイルから、対象の質問に対する回答を取得
        base_ans = base_llm_answers.get(question, "【回答なし】") # 回答がない場合は「回答なし」とする
        rag_ans = rag_answers.get(question, "【回答なし】")


        result_entry = {
            "Question": question,
            "Correct_Answer": correct_ans,
            "BaseLLM_Answer": base_ans,
            "RAG_Answer": rag_ans,
            # Placeholder for average scores
            "BaseLLM_Accuracy_Score_Avg": "N/A", "BaseLLM_Completeness_Score_Avg": "N/A", "BaseLLM_Relevance_Score_Avg": "N/A",
            "RAG_Accuracy_Score_Avg": "N/A", "RAG_Completeness_Score_Avg": "N/A", "RAG_Relevance_Score_Avg": "N/A",
            "BaseLLM_Evaluation_Notes": "", # 評価に関する注記やエラー情報
            "RAG_Evaluation_Notes": "",
        }

        if ENABLE_AUTOMATED_EVALUATION and evaluator_llm_client:
            prompt_templates = EVALUATION_PROMPT_TEMPLATES # 使用する全プロンプトテンプレート
            base_scores_list = [] # Base LLM回答のスコアリスト（各プロンプトの結果を格納）
            rag_scores_list = [] # RAG回答のスコアリスト
            base_reasons = [] # Base LLM回答の評価理由リスト
            rag_reasons = [] # RAG回答の評価理由リスト

            print(f"  -> Running automated evaluation using {len(prompt_templates)} prompts...")

            for prompt_idx, prompt_template in enumerate(prompt_templates):
                print(f"    -> Evaluating with Prompt Template {prompt_idx + 1}...")

                # Base LLM回答の評価 (リトライ付き)
                print("      -> BaseLLM...")
                eval_result = {"Error": "Not attempted"} # Initialize with an error state
                for attempt in range(MAX_RETRIES):
                    eval_result = automated_evaluate(question, base_ans, correct_ans, evaluator_llm_client, prompt_template, attempt)
                    if "Error" not in eval_result:
                        # 成功した場合、スコアをリストに追加
                        base_scores_list.append({
                            "Accuracy_Score": eval_result.get("Accuracy_Score", 0),
                            "Completeness_Score": eval_result.get("Completeness_Score", 0),
                            "Relevance_Score": eval_result.get("Relevance_Score", 0),
                        })
                        # 評価理由もリストに追加（短く表示）
                        base_reasons.append(f"P{prompt_idx+1} Attempt{attempt+1}: Acc({str(eval_result.get('Accuracy_Reason', 'N/A'))[:50]}), Comp({str(eval_result.get('Completeness_Reason', 'N/A'))[:50]}), Rel({str(eval_result.get('Relevance_Reason', 'N/A'))[:50]})") # 理由も短く表示、文字列化
                        print(f"        -> Success on attempt {attempt + 1}.")
                        break # 成功したらリトライループを抜ける
                    else:
                        # 失敗した場合
                        print(f"        -> Failed on attempt {attempt + 1} (Error: {eval_result.get('Error', 'Unknown')}).")
                        if attempt < MAX_RETRIES - 1:
                            # レート制限エラーの場合、少し長めに待つ
                            wait_time = RETRY_DELAY_BASE + random.uniform(0, 2) # 少しランダム性を加える
                            if eval_result.get("QuotaError"):
                                print(f"        -> Quota error, waiting {wait_time:.2f} seconds before retry...")
                                time.sleep(wait_time)
                            else:
                                # その他のエラーの場合は基本的な待機
                                print(f"        -> Waiting {API_CALL_DELAY:.2f} seconds before retry...")
                                time.sleep(API_CALL_DELAY)
                        # else: # 最終リトライも失敗した場合、理由リストへの追加はループの外で最後の結果を使用
                # リトライの結果に関わらず、最後の評価結果のエラー情報を記録 (成功時はエラーなし)
                if "Error" in eval_result:
                     base_reasons.append(f"P{prompt_idx+1} Final Error: {eval_result.get('Error', 'Unknown')[:100]}")


                # RAG回答の評価 (リトライ付き)
                print("      -> RAG...")
                eval_result = {"Error": "Not attempted"} # Initialize with an error state
                for attempt in range(MAX_RETRIES):
                    eval_result = automated_evaluate(question, rag_ans, correct_ans, evaluator_llm_client, prompt_template, attempt)
                    if "Error" not in eval_result:
                         rag_scores_list.append({
                            "Accuracy_Score": eval_result.get("Accuracy_Score", 0),
                            "Completeness_Score": eval_result.get("Completeness_Score", 0),
                            "Relevance_Score": eval_result.get("Relevance_Score", 0),
                        })
                         rag_reasons.append(f"P{prompt_idx+1} Attempt{attempt+1}: Acc({str(eval_result.get('Accuracy_Reason', 'N/A'))[:50]}), Comp({str(eval_result.get('Completeness_Reason', 'N/A'))[:50]}), Rel({str(eval_result.get('Relevance_Reason', 'N/A'))[:50]})")
                         print(f"        -> Success on attempt {attempt + 1}.")
                         break # 成功したらリトライループを抜ける
                    else:
                         print(f"        -> Failed on attempt {attempt + 1} (Error: {eval_result.get('Error', 'Unknown')}).")
                         if attempt < MAX_RETRIES - 1:
                            wait_time = RETRY_DELAY_BASE + random.uniform(0, 2) # 少しランダム性を加える
                            if eval_result.get("QuotaError"):
                                print(f"        -> Quota error, waiting {wait_time:.2f} seconds before retry...")
                                time.sleep(wait_time)
                            else:
                                print(f"        -> Waiting {API_CALL_DELAY:.2f} seconds before retry...")
                                time.sleep(API_CALL_DELAY)
                         # else: # 最終リトライも失敗した場合、理由リストへの追加はループの外で最後の結果を使用
                # リトライの結果に関わらず、最後の評価結果のエラー情報を記録 (成功時はエラーなし)
                if "Error" in eval_result:
                    rag_reasons.append(f"P{prompt_idx+1} Final Error: {eval_result.get('Error', 'Unknown')[:100]}")


                # --- プロンプト間のAPI呼び出し間隔 ---
                # 同じ質問・同じ回答者に対して、プロンプトを変えて呼び出す間の待機
                if prompt_idx < len(prompt_templates) - 1:
                     print(f"    -> Waiting {API_CALL_DELAY:.2f} seconds before next prompt...")
                     time.sleep(API_CALL_DELAY)


            # --- 平均スコアの計算と結果への格納 ---
            # base_scores_list には成功した評価のスコアのみが入っている
            if base_scores_list:
                # 成功した評価結果リストから平均を計算
                avg_base_scores = pd.DataFrame(base_scores_list).mean().to_dict()
                result_entry["BaseLLM_Accuracy_Score_Avg"] = round(avg_base_scores.get("Accuracy_Score", 0), 2) # 小数点第2位まで
                result_entry["BaseLLM_Completeness_Score_Avg"] = round(avg_base_scores.get("Completeness_Score", 0), 2)
                result_entry["BaseLLM_Relevance_Score_Avg"] = round(avg_base_scores.get("Relevance_Score", 0), 2)
                # 注記に、成功した評価の回数と理由リストを格納
                result_entry["BaseLLM_Evaluation_Notes"] = f"Avg over {len(base_scores_list)}/{len(prompt_templates)*MAX_RETRIES} total attempts. Reasons: {' | '.join(base_reasons)}"
            else:
                 # 全ての評価が失敗した場合
                 result_entry["BaseLLM_Evaluation_Notes"] = f"Eval failed for all {len(prompt_templates)*MAX_RETRIES} attempts. Reasons: {' | '.join(base_reasons)}"


            if rag_scores_list:
                # 成功した評価結果リストから平均を計算
                avg_rag_scores = pd.DataFrame(rag_scores_list).mean().to_dict()
                result_entry["RAG_Accuracy_Score_Avg"] = round(avg_rag_scores.get("Accuracy_Score", 0), 2)
                result_entry["RAG_Completeness_Score_Avg"] = round(avg_rag_scores.get("Completeness_Score", 0), 2)
                result_entry["RAG_Relevance_Score_Avg"] = round(avg_rag_scores.get("Relevance_Score", 0), 2)
                # 注記に、成功した評価の回数と理由リストを格納
                result_entry["RAG_Evaluation_Notes"] = f"Avg over {len(rag_scores_list)}/{len(prompt_templates)*MAX_RETRIES} total attempts. Reasons: {' | '.join(rag_reasons)}"
            else:
                # 全ての評価が失敗した場合
                result_entry["RAG_Evaluation_Notes"] = f"Eval failed for all {len(prompt_templates)*MAX_RETRIES} attempts. Reasons: {' | '.join(rag_reasons)}"

            # print(f"  -> Averaged Scores - BaseLLM: Acc={result_entry['BaseLLM_Accuracy_Score_Avg']}, Comp={result_entry['BaseLLM_Completeness_Score_Avg']}, Rel={result_entry['BaseLLM_Relevance_Score_Avg']}")
            # print(f"  -> Averaged Scores - RAG: Acc={result_entry['RAG_Accuracy_Score_Avg']}, Comp={result_entry['RAG_Completeness_Score_Avg']}, Rel={result_entry['RAG_Relevance_Score_Avg']}")

        # TODO: 必要なら手動評価をここに追加

        evaluation_results_list.append(result_entry) # この質問の結果をリストに追加

        # --- 質問間のAPI呼び出し間隔 ---
        # 1つの質問に対する評価を終えてから次の質問に移るまでの待機
        if i < len(questions_to_evaluate) - 1:
            print(f"\nWaiting {DELAY_BETWEEN_QUESTIONS:.2f} seconds before next question...")
            time.sleep(DELAY_BETWEEN_QUESTIONS)


    print("\n全ての質問の評価が完了しました。")

    # --- 4. 結果の保存 ---
    print(f"\n評価結果を '{OUTPUT_EVALUATION_FILE}' に保存します...")
    try:
        os.makedirs(OUTPUT_DIR, exist_ok=True)

        df_results = pd.DataFrame(evaluation_results_list)
        column_order = [
            "Question", "Correct_Answer", "BaseLLM_Answer", "RAG_Answer",
            "BaseLLM_Accuracy_Score_Avg", "BaseLLM_Completeness_Score_Avg", "BaseLLM_Relevance_Score_Avg",
            "RAG_Accuracy_Score_Avg", "RAG_Completeness_Score_Avg", "RAG_Relevance_Score_Avg",
            "BaseLLM_Evaluation_Notes", "RAG_Evaluation_Notes",
        ]
        df_results = df_results[ [col for col in column_order if col in df_results.columns] ]

        df_results.to_csv(OUTPUT_EVALUATION_FILE, index=False, encoding='utf-8')

        print(f"評価結果が正常に保存されました: {OUTPUT_EVALUATION_FILE}")

    except Exception as e:
        print(f"エラー: 評価結果の保存中にエラーが発生しました: {e}")